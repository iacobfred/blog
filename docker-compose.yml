x-default: &default
  deploy:
    restart_policy:
      condition: "${DOCKER_DEFAULT_RESTART_POLICY:-unless-stopped}"
  env_file:
    - .env

x-services: &services
  db:
    condition: service_healthy
  redis:
    condition: service_healthy
  jaeger:
    # TODO: add healthcheck
    condition: service_started

# https://docs.docker.com/compose/environment-variables/envvars/#compose_project_name
name: jacobjove

services:
  front:
    <<: *default
    image: ghcr.io/jacobjove/jacobjove:latest
    depends_on:
      - tempo
    ports:
      - ${PORT}:${PORT}
    environment:
      NEXTAUTH_URL_INTERNAL: "http://front:${PORT}"
      NEXTAUTH_URL: "https://jacobjove.org"
      NODE_ENV: production
  tempo:
    <<: *default
    image: "ghcr.io/sokaisolutions/tempo:latest"
    command:
      [
        "gunicorn",
        "--workers",
        "4",
        "--worker-class",
        "core.asgi.UvicornWorker",
        "core.asgi:application"
      ]
    environment:
      JAEGER_AGENT_HOST: jaeger
      POSTGRES_HOST: db
      PORT: ${PORT_API}
    depends_on:
      <<: *services
    ports:
      - ${PORT_API}:${PORT_API}
    restart: unless-stopped
    volumes:
      - ./_volumes/backups:/root/_volumes/backups
      - ./_volumes/media:/root/_volumes/media
      - ./_volumes/static:/root/_volumes/static
      - ./_volumes/logs:/root/_volumes/logs
      # - ./scripts/entrypoints/tempo.sh:/root/scripts/entrypoints/tempo.sh
      - ./.jwt_rsa.pem:/root/.jwt_rsa.pem
      - ./.env:/root/.env
  db:
    # https://registry.hub.docker.com/r/postgis/postgis/
    image: postgis/postgis:15-3.3-alpine
    expose:
      - 5432
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U $POSTGRES_USER" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    volumes:
      - db:/var/lib/postgresql/data
      - ./scripts/init/postgres.sh:/docker-entrypoint-initdb.d/postgres.sh
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}

  # dashboard:
  #   command: node packages/dashboard/server.mjs
  #   image: "ghcr.io/kwstriping/${APP}-dashboard:${SHA}"
  #   env_file:
  #     - .env
  #   environment:
  #     <<: *common-env-vars
  #     PORT: ${PORT_DASHBOARD}
  #   ports:
  #     - ${PORT_DASHBOARD}:${PORT_DASHBOARD}
  #   restart: unless-stopped
  #   depends_on:
  #     - tempo

  redis:
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 20s
      timeout: 10s
      retries: 3
      start_period: 20s
    image: library/redis:5.0-alpine
    expose:
      - 6379
    restart: unless-stopped
    volumes:
      - redis:/data

  worker:
    command: python manage.py run_huey -w 2
    depends_on:
      - redis
    image: "ghcr.io/sokaisolutions/tempo:${SHA}"
    env_file:
      - .env
    restart: unless-stopped
    volumes:
      - ./_volumes/media:/root/_volumes/media

  jaeger:
    image: jaegertracing/all-in-one
    expose:
      - "5775"
      - "6831"
      - "6832"
      - "5778"
      - "16686"
      - "14268"
      - "9411"
    restart: unless-stopped

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2
    restart: always
    ulimits:
      memlock:
        soft: -1
        hard: -1
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1024m -Xmx1024m"
      - ELASTIC_PASSWORD=changeme
      - xpack.security.enabled=true
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    healthcheck:
      interval: 10s
      retries: 12
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '"status":"red"'

  kibana:
    image: docker.elastic.co/kibana/kibana:7.16.2
    environment:
      ELASTICSEARCH_URL: "http://elasticsearch:9200"
      ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]'
      ELASTICSEARCH_USERNAME: elastic
      ELASTICSEARCH_PASSWORD: changeme
    restart: always
    depends_on:
      elasticsearch:
        condition: service_healthy
    expose:
      - "5601"

  apm-server:
    image: docker.elastic.co/apm/apm-server:7.16.2
    user: apm-server
    restart: always
    command:
      [
        "--strict.perms=false",
        "-e",
        "-E",
        "apm-server.host=0.0.0.0:8200",
        "-E",
        "apm-server.kibana.enabled=true",
        "-E",
        "apm-server.kibana.host=kibana:5601",
        "-E",
        "apm-server.kibana.username=elastic",
        "-E",
        "apm-server.kibana.password=changeme",
        "-E",
        "output.elasticsearch.hosts=['elasticsearch:9200']",
        "-E",
        "output.elasticsearch.enabled=true",
        "-E",
        "output.elasticsearch.username=elastic",
        "-E",
        "output.elasticsearch.password=changeme"
      ]
    depends_on:
      elasticsearch:
        condition: service_healthy
    cap_add: [ "CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID" ]
    cap_drop: [ "ALL" ]
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/

  otel-collector:
    image: otel/opentelemetry-collector:0.41.0
    restart: always
    command: "--config=/etc/otel-collector-config.yaml"
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    depends_on:
      apm-server:
        condition: service_healthy
    expose:
      - "4317"

volumes:
  db:
    driver: local
  redis:
    driver: local
  elasticsearch-data:
    driver: local
